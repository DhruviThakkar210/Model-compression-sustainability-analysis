# Model-compression-sustainability-analysis
Comparative study on the environmental impact of model compression techniques â€” Quantization, Pruning, and Knowledge Distillation â€” applied to DeepSeek-Coder 1.3B using the CodeCarbon framework. 
# ğŸ§  Model Compression and Green AI  
### *Comparative Environmental Impact of Model Compression Techniques*  
*Author: Dhruvi Rajeshkumar Thakkar*  
*Course: AI for Sustainable Computing â€” University of Calgary*

---

## ğŸ“˜ Overview  
This repository presents **Assignment 2**, which explores the environmental efficiency of three model-compression techniques â€” **Quantization**, **Structured Pruning**, and **Knowledge Distillation** â€” applied to the `DeepSeek-Coder-1.3B` transformer model.  

The study uses the **CodeCarbon** framework to measure energy usage and COâ‚‚ emissions during inference.  
The objective is to analyze the trade-off between **sustainability (energy/emission savings)** and **model performance**.

---

## âš™ï¸ Methods  
Each model was tested using the same benchmark task â€” **Python docstring generation** â€” to ensure consistent computational load.  

| Compression Method | Description |
|:--|:--|
| **Baseline (FP32)** | Uncompressed DeepSeek-Coder 1.3B model in full precision. |
| **Quantized (8-bit)** | Model weights reduced to 8-bit precision using BitsAndBytes. |
| **Structured Pruned (20%)** | 20 % of Linear layer weights removed via L2-norm structured pruning. |
| **Distilled (Tiny GPT-2)** | Lightweight student model trained to mimic the DeepSeek teacher. |

Each configuration was executed for 3â€“5 epochs using CodeCarbonâ€™s emission tracker.

---

## ğŸ“Š Results Summary  

| Model | Compression | Size (MB) | COâ‚‚ (kg eq) | Energy (kWh) | Latency (s) | Accuracy (P/F) |
|:--|:--|:--:|:--:|:--:|:--:|:--:|
| Baseline | FP32 | 1300 | 0.00150 | 0.00040 | 14.2 | F |
| Quantized | 8-bit | 350 | 0.00110 | 0.00031 | 9.8 | F |
| Pruned | 20 % Structured | 600 | 0.00120 | 0.00033 | 10.5 | F |
| Distilled | Tiny GPT-2 | **30** | **0.00065** | **0.00018** | **2.5** | F |

**Key Findings**
- Model size and COâ‚‚ emissions are strongly inversely correlated (**r â‰ˆ â€“0.91**).  
- Distillation achieves the **lowest emissions (0.00065 kg COâ‚‚)** and **fastest latency (2.5 s)**.  
- Quantization is the most practical real-world method for quick emission savings.  
- All models failed the heuristic docstring accuracy check but successfully demonstrate environmental benefits.

---

## ğŸŒ± Visualization  

**Figure 1 â€” COâ‚‚ Emissions by Model**  
*Emission drops steadily with compression level.*

**Figure 2 â€” Latency by Model**  
*Distilled model performs 6Ã— faster than baseline.*


---

## ğŸ§® Code Structure  

â”œâ”€â”€ baseline_results.py
â”œâ”€â”€ quantized_results.py
â”œâ”€â”€ pruned_results.py
â”œâ”€â”€ distilled_results.py
â”œâ”€â”€ emissions_summary.csv
â””â”€â”€ Assignment2_ModelCompression_SustainabilityReport.pdf
