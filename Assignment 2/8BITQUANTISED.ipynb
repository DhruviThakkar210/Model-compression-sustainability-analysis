{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a60269bb3b854d41a89fc6ef426fef86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9bf02b68b4f49f9b6ae6a0c4ecacd8c",
              "IPY_MODEL_9750b2a0701f4c2b805746d7c75e5afc",
              "IPY_MODEL_c4139b19fa6b4867a8aecb35fb0ff117"
            ],
            "layout": "IPY_MODEL_8d2a1820f222462eaa43f0bdf7da2e55"
          }
        },
        "d9bf02b68b4f49f9b6ae6a0c4ecacd8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95fa9336ba4149178f568083a8e02ef9",
            "placeholder": "​",
            "style": "IPY_MODEL_52717aab81c8493f85aa0ab64989f91e",
            "value": ""
          }
        },
        "9750b2a0701f4c2b805746d7c75e5afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f119eaf592d48ed8b6775cae052e924",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58817a8d86294900ad0bb8e9507c3514",
            "value": 0
          }
        },
        "c4139b19fa6b4867a8aecb35fb0ff117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a41646df9c1f4bc58cfebc5b84628270",
            "placeholder": "​",
            "style": "IPY_MODEL_e2aa17876c1244d89d742e09b7aac3b1",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "8d2a1820f222462eaa43f0bdf7da2e55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95fa9336ba4149178f568083a8e02ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52717aab81c8493f85aa0ab64989f91e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f119eaf592d48ed8b6775cae052e924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "58817a8d86294900ad0bb8e9507c3514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a41646df9c1f4bc58cfebc5b84628270": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2aa17876c1244d89d742e09b7aac3b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a53fa7d756484568a813108bb48778f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bac417b050045f0bb6ae2b11b0d4c63",
              "IPY_MODEL_b94f936f9dc740ea8a265a7f0fed3c24",
              "IPY_MODEL_3c7e1966154245d3b62a408c23840083"
            ],
            "layout": "IPY_MODEL_553f4957a4e64b38bc8c646286c8634c"
          }
        },
        "3bac417b050045f0bb6ae2b11b0d4c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_192b7c33805c49e2af5348551346ce43",
            "placeholder": "​",
            "style": "IPY_MODEL_34e136468a8c4edab9cd8f28d21bba2b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "b94f936f9dc740ea8a265a7f0fed3c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5e9223d455642089317eca442823327",
            "max": 2693014838,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73d868548484431a822c176616ce1cb7",
            "value": 2693014838
          }
        },
        "3c7e1966154245d3b62a408c23840083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e149edcf850b47119d195a918c44d3fb",
            "placeholder": "​",
            "style": "IPY_MODEL_10c2ca0a224a4446bae44b728f466ec2",
            "value": " 2.69G/2.69G [00:36&lt;00:00, 134MB/s]"
          }
        },
        "553f4957a4e64b38bc8c646286c8634c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "192b7c33805c49e2af5348551346ce43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34e136468a8c4edab9cd8f28d21bba2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5e9223d455642089317eca442823327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d868548484431a822c176616ce1cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e149edcf850b47119d195a918c44d3fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10c2ca0a224a4446bae44b728f466ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9118e3ba0e6e4b53b02714fd3ba3cb87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42f27ad417ca49c29fe2823632b74f3f",
              "IPY_MODEL_879afcdacd054396bd46b50cba31a0ab",
              "IPY_MODEL_3629909954f84ce29449cfe8fabdf0f3"
            ],
            "layout": "IPY_MODEL_336e4b6218934d42ab773b20947ea396"
          }
        },
        "42f27ad417ca49c29fe2823632b74f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_115e76f729d04455abcb346193826c12",
            "placeholder": "​",
            "style": "IPY_MODEL_be84e6d9a13844b1a617e317383f305d",
            "value": "generation_config.json: 100%"
          }
        },
        "879afcdacd054396bd46b50cba31a0ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb53dea965844344b6d508ae0269834e",
            "max": 119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27bf4c1bfa04483c98320c48b5ae1e3c",
            "value": 119
          }
        },
        "3629909954f84ce29449cfe8fabdf0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9e4da7ca77e447393483aa4ca3bd0b0",
            "placeholder": "​",
            "style": "IPY_MODEL_9ae16ad09c8742549692e6db64c1ddcf",
            "value": " 119/119 [00:00&lt;00:00, 11.3kB/s]"
          }
        },
        "336e4b6218934d42ab773b20947ea396": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115e76f729d04455abcb346193826c12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be84e6d9a13844b1a617e317383f305d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb53dea965844344b6d508ae0269834e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27bf4c1bfa04483c98320c48b5ae1e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9e4da7ca77e447393483aa4ca3bd0b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ae16ad09c8742549692e6db64c1ddcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣  Clean up and reinstall missing dependencies\n",
        "!pip install -U triton==2.3.0 bitsandbytes==0.43.1 accelerate transformers codecarbon\n",
        "\n",
        "# 2️⃣  Restart runtime (important!)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GHQOsk8Yi7N",
        "outputId": "230e01cf-3c77-45b7-9125-435250258051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==2.3.0\n",
            "  Downloading triton-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: bitsandbytes==0.43.1 in /usr/local/lib/python3.12/dist-packages (0.43.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: codecarbon in /usr/local/lib/python3.12/dist-packages (3.0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from triton==2.3.0) (3.20.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.43.1) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.43.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.12/dist-packages (from codecarbon) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from codecarbon) (8.3.0)\n",
            "Requirement already satisfied: fief-client[cli] in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.23.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.11.10)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from codecarbon) (12.575.51)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from codecarbon) (3.14.1)\n",
            "Requirement already satisfied: questionary in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.1.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (1.11.1.6)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from bitsandbytes==0.43.1)\n",
            "  Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting torch (from bitsandbytes==0.43.1)\n",
            "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "  Downloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting torch (from bitsandbytes==0.43.1)\n",
            "  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting torch (from bitsandbytes==0.43.1)\n",
            "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting torch (from bitsandbytes==0.43.1)\n",
            "  Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Downloading torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from arrow->codecarbon) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from arrow->codecarbon) (2025.2)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n",
            "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon) (1.5.6)\n",
            "Requirement already satisfied: yaspin in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon) (3.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->codecarbon) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->codecarbon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->codecarbon) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->codecarbon) (0.4.2)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from questionary->codecarbon) (3.0.52)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->codecarbon) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->codecarbon) (2.19.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.16.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.12/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->bitsandbytes==0.43.1) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->bitsandbytes==0.43.1) (1.3.0)\n",
            "Requirement already satisfied: termcolor<4.0,>=3.1 in /usr/local/lib/python3.12/dist-packages (from yaspin->fief-client[cli]->codecarbon) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.23)\n",
            "Downloading triton-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.1 triton-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, bitsandbytes as bnb, triton\n",
        "\n",
        "print(\"✅ CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
        "print(\"BitsAndBytes version:\", bnb.__version__)\n",
        "print(\"Triton version:\", triton.__version__)\n",
        "print(\"Torch version:\", torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDldUtDkZlfO",
        "outputId": "9edbcecb-ee3f-48d8-b2e6-8153dd7c210c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "BitsAndBytes version: 0.43.1\n",
            "Triton version: 2.3.0\n",
            "Torch version: 2.3.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers==4.41.2 accelerate==0.30.1 bitsandbytes==0.43.1 codecarbon==3.0.7\n"
      ],
      "metadata": {
        "id": "lJzCoqBWaMEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, bitsandbytes, torch\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"BitsAndBytes:\", bitsandbytes.__version__)\n",
        "print(\"Torch:\", torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "a60269bb3b854d41a89fc6ef426fef86",
            "d9bf02b68b4f49f9b6ae6a0c4ecacd8c",
            "9750b2a0701f4c2b805746d7c75e5afc",
            "c4139b19fa6b4867a8aecb35fb0ff117",
            "8d2a1820f222462eaa43f0bdf7da2e55",
            "95fa9336ba4149178f568083a8e02ef9",
            "52717aab81c8493f85aa0ab64989f91e",
            "3f119eaf592d48ed8b6775cae052e924",
            "58817a8d86294900ad0bb8e9507c3514",
            "a41646df9c1f4bc58cfebc5b84628270",
            "e2aa17876c1244d89d742e09b7aac3b1"
          ]
        },
        "id": "iSsRBUkEaw9G",
        "outputId": "57893ae6-f164-4269-ebce-99189337aabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a60269bb3b854d41a89fc6ef426fef86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers: 4.41.2\n",
            "BitsAndBytes: 0.43.1\n",
            "Torch: 2.3.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a53fa7d756484568a813108bb48778f2",
            "3bac417b050045f0bb6ae2b11b0d4c63",
            "b94f936f9dc740ea8a265a7f0fed3c24",
            "3c7e1966154245d3b62a408c23840083",
            "553f4957a4e64b38bc8c646286c8634c",
            "192b7c33805c49e2af5348551346ce43",
            "34e136468a8c4edab9cd8f28d21bba2b",
            "d5e9223d455642089317eca442823327",
            "73d868548484431a822c176616ce1cb7",
            "e149edcf850b47119d195a918c44d3fb",
            "10c2ca0a224a4446bae44b728f466ec2",
            "9118e3ba0e6e4b53b02714fd3ba3cb87",
            "42f27ad417ca49c29fe2823632b74f3f",
            "879afcdacd054396bd46b50cba31a0ab",
            "3629909954f84ce29449cfe8fabdf0f3",
            "336e4b6218934d42ab773b20947ea396",
            "115e76f729d04455abcb346193826c12",
            "be84e6d9a13844b1a617e317383f305d",
            "cb53dea965844344b6d508ae0269834e",
            "27bf4c1bfa04483c98320c48b5ae1e3c",
            "b9e4da7ca77e447393483aa4ca3bd0b0",
            "9ae16ad09c8742549692e6db64c1ddcf"
          ]
        },
        "id": "2_gThTpHT8fI",
        "outputId": "ba9ce81d-a62e-4cde-e672-7db1a639eaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n",
            "Loading model in 8-bit mode...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a53fa7d756484568a813108bb48778f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9118e3ba0e6e4b53b02714fd3ba3cb87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 18:12:49] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 18:12:49] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 18:12:49] [setup] CPU Tracking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 18:12:51] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 18:12:51] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 18:12:51] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 18:12:51] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 18:12:51] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 18:12:51] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 18:12:51] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 18:12:51] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 18:12:51]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 18:12:51]   Python version: 3.12.12\n",
            "[codecarbon INFO @ 18:12:51]   CodeCarbon version: 3.0.7\n",
            "[codecarbon INFO @ 18:12:51]   Available RAM : 12.671 GB\n",
            "[codecarbon INFO @ 18:12:51]   CPU count: 2 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 18:12:51]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 18:12:51]   GPU count: 1\n",
            "[codecarbon INFO @ 18:12:51]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 18:12:51] Emissions data (if any) will be saved to file /content/results/emissions.csv\n",
            "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n",
            "[codecarbon INFO @ 18:13:06] Energy consumed for RAM : 0.000042 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:13:06] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:13:06] Energy consumed for All CPU : 0.000177 kWh\n",
            "[codecarbon INFO @ 18:13:06] Energy consumed for all GPUs : 0.000118 kWh. Total GPU Power : 28.218705895772928 W\n",
            "[codecarbon INFO @ 18:13:06] 0.000336 kWh of electricity used since the beginning.\n",
            "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: 18.82s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: 9.40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 18:13:21] Energy consumed for RAM : 0.000083 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:13:21] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:13:21] Energy consumed for All CPU : 0.000354 kWh\n",
            "[codecarbon INFO @ 18:13:21] Energy consumed for all GPUs : 0.000243 kWh. Total GPU Power : 30.010188075311223 W\n",
            "[codecarbon INFO @ 18:13:21] 0.000680 kWh of electricity used since the beginning.\n",
            "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: 8.62s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 18:13:36] Energy consumed for RAM : 0.000125 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:13:36] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:13:36] Energy consumed for All CPU : 0.000531 kWh\n",
            "[codecarbon INFO @ 18:13:36] Energy consumed for all GPUs : 0.000367 kWh. Total GPU Power : 29.805541689698202 W\n",
            "[codecarbon INFO @ 18:13:36] 0.001023 kWh of electricity used since the beginning.\n",
            "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: 10.35s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 18:13:48] Energy consumed for RAM : 0.000157 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:13:48] Delta energy consumed for CPU with constant : 0.000137 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:13:48] Energy consumed for All CPU : 0.000668 kWh\n",
            "[codecarbon INFO @ 18:13:48] Energy consumed for all GPUs : 0.000464 kWh. Total GPU Power : 30.085966778031974 W\n",
            "[codecarbon INFO @ 18:13:48] 0.001289 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: 9.41s\n",
            "\n",
            "✅ Quantized run complete! Results saved to:\n",
            "/content/results/quantized_results.csv\n",
            "⏱️ Avg Latency: 11.32s | 🌱 CO₂ Emission: 0.000179 kg\n"
          ]
        }
      ],
      "source": [
        "# ⚡ QUANTIZED MODEL BLOCK (8-bit, FINAL STABLE VERSION)\n",
        "from codecarbon import EmissionsTracker\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch, time, csv, os\n",
        "from datetime import datetime\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on:\", device)\n",
        "\n",
        "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "output_path = \"/content/results\"\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "csv_path = os.path.join(output_path, \"quantized_results.csv\")\n",
        "\n",
        "code_sample = \"\"\"from typing import List\n",
        "def below_zero(operations: List[int]) -> bool:\n",
        "    balance = 0\n",
        "    for op in operations:\n",
        "        balance += op\n",
        "        if balance < 0:\n",
        "            return True\n",
        "    return False\n",
        "\"\"\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
        "\n",
        "print(\"Loading model in 8-bit mode...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ").eval()\n",
        "\n",
        "print(\"✅ Model loaded successfully!\")\n",
        "\n",
        "tracker = EmissionsTracker(output_dir=output_path, project_name=\"Quantized_Run\")\n",
        "tracker.start()\n",
        "\n",
        "latencies = []\n",
        "for epoch in range(1, 6):\n",
        "    start = time.time()\n",
        "    inputs = tokenizer(code_sample, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=64)\n",
        "    latency = time.time() - start\n",
        "    latencies.append(latency)\n",
        "    print(f\"Epoch {epoch}: {latency:.2f}s\")\n",
        "\n",
        "total_emission = tracker.stop()\n",
        "avg_latency = sum(latencies) / len(latencies)\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "with open(csv_path, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Model\", \"Quantization\", \"Avg_Latency_s\", \"CO2_kg\", \"Timestamp\"])\n",
        "    writer.writerow([\"DeepSeek\", \"8-bit\", avg_latency, total_emission, timestamp])\n",
        "\n",
        "print(f\"\\n✅ Quantized run complete! Results saved to:\\n{csv_path}\")\n",
        "print(f\"⏱️ Avg Latency: {avg_latency:.2f}s | 🌱 CO₂ Emission: {total_emission:.6f} kg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers bitsandbytes accelerate\n",
        "!pip install transformers==4.39.3 bitsandbytes==0.43.0 accelerate==0.27.2 codecarbon\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BUanERXmf1oD",
        "outputId": "7b175050-9073-4906-de97-eaf930544fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.57.1\n",
            "Uninstalling transformers-4.57.1:\n",
            "  Successfully uninstalled transformers-4.57.1\n",
            "Found existing installation: bitsandbytes 0.43.1\n",
            "Uninstalling bitsandbytes-0.43.1:\n",
            "  Successfully uninstalled bitsandbytes-0.43.1\n",
            "Found existing installation: accelerate 1.11.0\n",
            "Uninstalling accelerate-1.11.0:\n",
            "  Successfully uninstalled accelerate-1.11.0\n",
            "Collecting transformers==4.39.3\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.43.0\n",
            "  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting accelerate==0.27.2\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: codecarbon in /usr/local/lib/python3.12/dist-packages (3.0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.39.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.39.3) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.39.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.39.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.39.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.39.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.39.3) (2.32.4)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3)\n",
            "  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.39.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.39.3) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.43.0) (2.3.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.27.2) (7.1.1)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.12/dist-packages (from codecarbon) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from codecarbon) (8.3.0)\n",
            "Requirement already satisfied: fief-client[cli] in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.23.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.11.10)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from codecarbon) (12.575.51)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from codecarbon) (3.14.1)\n",
            "Requirement already satisfied: questionary in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.1.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (1.1.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes==0.43.0) (12.6.85)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from arrow->codecarbon) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from arrow->codecarbon) (2025.2)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n",
            "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon) (1.5.6)\n",
            "Requirement already satisfied: yaspin in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon) (3.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->codecarbon) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->codecarbon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->codecarbon) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->codecarbon) (0.4.2)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from questionary->codecarbon) (3.0.52)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.39.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.39.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.39.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.39.3) (2025.10.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->codecarbon) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->codecarbon) (2.19.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.16.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.12/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->bitsandbytes==0.43.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->bitsandbytes==0.43.0) (1.3.0)\n",
            "Requirement already satisfied: termcolor<4.0,>=3.1 in /usr/local/lib/python3.12/dist-packages (from yaspin->fief-client[cli]->codecarbon) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.23)\n",
            "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, bitsandbytes, accelerate\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.27.2 bitsandbytes-0.43.0 tokenizers-0.15.2 transformers-4.39.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "accelerate",
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "e7f62acd1b724be79c8fe769dc166aa8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, bitsandbytes as bnb\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "print(\"✅ CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")\n",
        "print(\"BitsAndBytes version:\", bnb.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVA7-H5dggEM",
        "outputId": "b4779829-473a-4628-8f2a-3037f3e244f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "BitsAndBytes version: 0.43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# ⚡ QUANTIZED MODEL BLOCK (8-bit, FINAL FULL VERSION)\n",
        "# ===============================================================\n",
        "\n",
        "from codecarbon import EmissionsTracker\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch, time, csv, os\n",
        "from datetime import datetime\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on:\", device)\n",
        "\n",
        "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "output_path = \"/content/results\"\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "csv_path = os.path.join(output_path, \"quantized_results.csv\")\n",
        "\n",
        "# ===============================================================\n",
        "# 🧩 SAMPLE INPUT + PROMPT\n",
        "# ===============================================================\n",
        "code_sample = \"\"\"from typing import List\n",
        "def below_zero(operations: List[int]) -> bool:\n",
        "    balance = 0\n",
        "    for op in operations:\n",
        "        balance += op\n",
        "        if balance < 0:\n",
        "            return True\n",
        "    return False\n",
        "\"\"\"\n",
        "\n",
        "prompt_text = (\n",
        "    \"You are an expert Python developer. \"\n",
        "    \"Given the function below, generate a complete and well-formatted Google-style docstring. \"\n",
        "    \"Include purpose, parameters, returns, and one example. \"\n",
        "    \"Provide only the full function code with the docstring.\\n\\n\"\n",
        "    f\"{code_sample}\"\n",
        ")\n",
        "\n",
        "# ===============================================================\n",
        "# ⚙️ HELPER FUNCTIONS\n",
        "# ===============================================================\n",
        "def evaluate_accuracy(text):\n",
        "    checks = ['\"\"\"', \"Args:\", \"Returns:\"]\n",
        "    return \"P\" if all(x in text for x in checks) else \"F\"\n",
        "\n",
        "def get_model_size_mb(model):\n",
        "    torch.save(model.state_dict(), \"temp.pt\")\n",
        "    size = os.path.getsize(\"temp.pt\") / (1024 * 1024)\n",
        "    os.remove(\"temp.pt\")\n",
        "    return round(size, 2)\n",
        "\n",
        "# ===============================================================\n",
        "# 🧠 LOAD MODEL (8-bit)\n",
        "# ===============================================================\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
        "\n",
        "print(\"Loading model in 8-bit mode...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ").eval()\n",
        "\n",
        "print(\"✅ Model loaded successfully!\")\n",
        "\n",
        "# ===============================================================\n",
        "# 🌱 EMISSION TRACKER + EXPERIMENT LOOP\n",
        "# ===============================================================\n",
        "EPOCHS = 5\n",
        "tracker = EmissionsTracker(output_dir=output_path, project_name=\"Quantized_Run\")\n",
        "\n",
        "# prepare CSV\n",
        "if not os.path.exists(csv_path):\n",
        "    with open(csv_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"Model\", \"Quantization\", \"Epoch\", \"CO2_kg\",\n",
        "            \"Latency_s\", \"Model_Size_MB\", \"Accuracy\", \"Timestamp\"\n",
        "        ])\n",
        "\n",
        "# main loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(f\"\\n⚡ Epoch {epoch}/{EPOCHS}...\")\n",
        "    tracker.start()\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.3,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    latency = round(time.perf_counter() - start, 3)\n",
        "    co2 = tracker.stop()\n",
        "\n",
        "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    acc = evaluate_accuracy(text)\n",
        "    size = get_model_size_mb(model)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    with open(csv_path, \"a\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow(\n",
        "            [\"DeepSeek\", \"8-bit\", epoch, co2, latency, size, acc, timestamp]\n",
        "        )\n",
        "\n",
        "print(f\"\\n✅ Quantized experiment complete. Results saved to: {csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMGqkhlPgvb8",
        "outputId": "65a94478-3ae4-408f-b85b-4928e131a4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n",
            "Loading model in 8-bit mode...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[codecarbon WARNING @ 18:37:59] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 18:37:59] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 18:37:59] [setup] CPU Tracking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 18:38:00] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 18:38:00] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 18:38:00] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 18:38:00] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 18:38:00] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 18:38:00] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 18:38:00] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 18:38:00] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 18:38:00]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 18:38:00]   Python version: 3.12.12\n",
            "[codecarbon INFO @ 18:38:00]   CodeCarbon version: 3.0.7\n",
            "[codecarbon INFO @ 18:38:00]   Available RAM : 12.671 GB\n",
            "[codecarbon INFO @ 18:38:00]   CPU count: 2 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 18:38:00]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 18:38:00]   GPU count: 1\n",
            "[codecarbon INFO @ 18:38:00]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 18:38:00] Emissions data (if any) will be saved to file /content/results/emissions.csv\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚡ Epoch 1/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 18:38:15] Energy consumed for RAM : 0.000042 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:38:15] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:38:15] Energy consumed for All CPU : 0.000177 kWh\n",
            "[codecarbon INFO @ 18:38:15] Energy consumed for all GPUs : 0.000124 kWh. Total GPU Power : 29.804649019056832 W\n",
            "[codecarbon INFO @ 18:38:15] 0.000343 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 18:38:30] Energy consumed for RAM : 0.000083 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:38:30] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:38:30] Energy consumed for All CPU : 0.000354 kWh\n",
            "[codecarbon INFO @ 18:38:30] Energy consumed for all GPUs : 0.000256 kWh. Total GPU Power : 31.73203088135635 W\n",
            "[codecarbon INFO @ 18:38:30] 0.000694 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 18:38:40] Energy consumed for RAM : 0.000112 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:38:40] Delta energy consumed for CPU with constant : 0.000121 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:38:40] Energy consumed for All CPU : 0.000475 kWh\n",
            "[codecarbon INFO @ 18:38:40] Energy consumed for all GPUs : 0.000347 kWh. Total GPU Power : 31.779102052612092 W\n",
            "[codecarbon INFO @ 18:38:40] 0.000934 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 18:38:51] Already started tracking\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚡ Epoch 2/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "[codecarbon WARNING @ 18:39:21] Tracker already stopped !\n",
            "[codecarbon INFO @ 18:39:21] Energy consumed for RAM : 0.000224 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:39:21] Delta energy consumed for CPU with constant : 0.000476 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:39:21] Energy consumed for All CPU : 0.000951 kWh\n",
            "[codecarbon INFO @ 18:39:21] Energy consumed for all GPUs : 0.000701 kWh. Total GPU Power : 31.633562790385234 W\n",
            "[codecarbon INFO @ 18:39:21] 0.001877 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 18:39:28] Already started tracking\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚡ Epoch 3/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "[codecarbon WARNING @ 18:39:59] Tracker already stopped !\n",
            "[codecarbon INFO @ 18:39:59] Energy consumed for RAM : 0.000329 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:39:59] Delta energy consumed for CPU with constant : 0.000447 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:39:59] Energy consumed for All CPU : 0.001398 kWh\n",
            "[codecarbon INFO @ 18:39:59] Energy consumed for all GPUs : 0.001040 kWh. Total GPU Power : 32.23345149869592 W\n",
            "[codecarbon INFO @ 18:39:59] 0.002768 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 18:40:10] Already started tracking\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚡ Epoch 4/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "[codecarbon WARNING @ 18:40:40] Tracker already stopped !\n",
            "[codecarbon INFO @ 18:40:40] Energy consumed for RAM : 0.000444 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:40:40] Delta energy consumed for CPU with constant : 0.000489 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:40:40] Energy consumed for All CPU : 0.001887 kWh\n",
            "[codecarbon INFO @ 18:40:40] Energy consumed for all GPUs : 0.001413 kWh. Total GPU Power : 32.40991416814337 W\n",
            "[codecarbon INFO @ 18:40:40] 0.003744 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 18:40:53] Already started tracking\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚡ Epoch 5/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "[codecarbon WARNING @ 18:41:23] Tracker already stopped !\n",
            "[codecarbon INFO @ 18:41:23] Energy consumed for RAM : 0.000563 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:41:23] Delta energy consumed for CPU with constant : 0.000505 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:41:23] Energy consumed for All CPU : 0.002392 kWh\n",
            "[codecarbon INFO @ 18:41:23] Energy consumed for all GPUs : 0.001804 kWh. Total GPU Power : 32.890197267914346 W\n",
            "[codecarbon INFO @ 18:41:23] 0.004758 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Quantized experiment complete. Results saved to: /content/results/quantized_results.csv\n"
          ]
        }
      ]
    }
  ]
}